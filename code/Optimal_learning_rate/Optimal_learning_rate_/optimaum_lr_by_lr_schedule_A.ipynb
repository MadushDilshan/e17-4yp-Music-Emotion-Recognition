{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"639qYT1dutm-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694922766554,"user_tz":-330,"elapsed":3112235,"user":{"displayName":"HASARA DASANMI WIJESOORIYA","userId":"13451757717494225581"}},"outputId":"bb75bce5-b519-4c3b-f624-5780779a91b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 1/55\n","3317/3317 [==============================] - 57s 17ms/step - loss: 0.0800\n","Epoch 2/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 3/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 4/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 5/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 6/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 7/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 8/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 9/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 10/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 11/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 12/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 13/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 14/55\n","3317/3317 [==============================] - 55s 16ms/step - loss: 0.0799\n","Epoch 15/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 16/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 17/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 18/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 19/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 20/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 21/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 22/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 23/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 24/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 25/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 26/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 27/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 28/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 29/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 30/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 31/55\n","3317/3317 [==============================] - 55s 16ms/step - loss: 0.0799\n","Epoch 32/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 33/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 34/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 35/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 36/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 37/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 38/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 39/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 40/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 41/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 42/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 43/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 44/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 45/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 46/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 47/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 48/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 49/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 50/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 51/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 52/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Epoch 53/55\n","3317/3317 [==============================] - 56s 17ms/step - loss: 0.0799\n","Epoch 54/55\n","3317/3317 [==============================] - 56s 17ms/step - loss: 0.0799\n","Epoch 55/55\n","3317/3317 [==============================] - 55s 17ms/step - loss: 0.0799\n","Optimal Learning Rate: 0.001\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import glob\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Bidirectional, Flatten, Dense\n","from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Define the path to the directory containing the feature files\n","directory_path = '/content/drive/MyDrive/FYP_dataset/features'\n","\n","# Get a list of all feature file paths in the directory\n","feature_files = glob.glob(directory_path + '/*.csv')\n","\n","# Define lists to store the feature and target data\n","X = []\n","y_a = []\n","\n","# Iterate over each feature file\n","for file in feature_files:\n","    # Read the feature file into a DataFrame\n","    df = pd.read_csv(file)\n","\n","    # Extract the features and target variables\n","    features = df.drop(['Arousal_Value', 'frameTime','Valence_Value'], axis=1).values\n","    arousal = df['Arousal_Value'].values\n","\n","    # Append the data to the lists\n","    X.append(features)\n","    y_a.append(arousal)\n","\n","# Concatenate the feature and target arrays\n","X = np.concatenate(X)\n","y_a = np.concatenate(y_a)\n","\n","# Scale the features\n","scaler = MinMaxScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Reshape the input data for LSTM\n","X_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n","\n","# Define a custom learning rate schedule\n","class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, initial_learning_rate, decay_steps, decay_rate):\n","        super(CustomLearningRateSchedule, self).__init__()\n","        self.initial_learning_rate = initial_learning_rate\n","        self.decay_steps = decay_steps\n","        self.decay_rate = decay_rate\n","\n","    def __call__(self, step):\n","        learning_rate = self.initial_learning_rate * self.decay_rate ** (step / self.decay_steps)\n","        return learning_rate\n","\n","# Create a custom learning rate schedule\n","lr_schedule = CustomLearningRateSchedule(initial_learning_rate=0.001, decay_steps=10, decay_rate=0.1)\n","\n","# Build the BiLSTM model for Arousal\n","model_a = Sequential()\n","model_a.add(Bidirectional(CuDNNLSTM(64, return_sequences=True), input_shape=(X_reshaped.shape[1], 1)))\n","model_a.add(Flatten())\n","model_a.add(Dense(1))\n","model_a.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n","\n","# Train the Arousal model\n","model_a.fit(X_reshaped, y_a, epochs=55, batch_size=32)\n","\n","# Select the optimal learning rate\n","optimal_lr = lr_schedule(0)  # Evaluate the learning rate at the first step\n","print(f'Optimal Learning Rate: {optimal_lr}')\n"]}]}